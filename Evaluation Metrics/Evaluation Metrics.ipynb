{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7479aef5",
   "metadata": {},
   "source": [
    "# Comprehensive Guide to LLM/GenAI Evaluation Metrics\n",
    "\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- Text Generation Metrics\n",
    "- Semantic Similarity Metrics\n",
    "- Language Quality Metrics\n",
    "- Task-Specific Metrics\n",
    "- Human Evaluation Metrics\n",
    "\n",
    "\n",
    "## 1. Text Generation Metrics\n",
    "\n",
    "### BLEU (Bilingual Evaluation Understudy) Score\n",
    "\n",
    "**Definition**  \n",
    "BLEU measures how similar AI-generated text is to human-written reference text by comparing overlapping n-grams (sequences of words).\n",
    "\n",
    "**Formula**  \n",
    "$$\\text{BLEU} = \\text{BP} \\times \\exp\\left(\\sum (\\text{w}_n \\times \\log \\text{p}_n)\\right)$$\n",
    "\n",
    "Where:\n",
    "- **BP** = Brevity Penalty\n",
    "- **w_n** = Weight for n-gram\n",
    "- **p_n** = Modified precision for n-gram\n",
    "\n",
    "**Example**  \n",
    "Reference: \"The cat is on the mat\"  \n",
    "AI Output: \"The cat sits on the mat\"  \n",
    "BLEU Score: 0.83 (high similarity)\n",
    "\n",
    "**Use Cases**\n",
    "- Machine translation evaluation\n",
    "- Text summarization quality\n",
    "- Content generation assessment\n",
    "\n",
    "### Perplexity\n",
    "\n",
    "**Definition**  \n",
    "Perplexity is a metric used to evaluate how well a language model predicts a sample of text. It essentially measures the uncertainty of the model when predicting the next word in a sequence. Lower perplexity indicates better performance, meaning the model is more confident in its predictions.\n",
    "\n",
    "**Formula**  \n",
    "The formula for perplexity is:  \n",
    "$$\\text{Perplexity} = 2^{\\left(-\\frac{1}{N} \\sum \\log_2 P(x_i)\\right)}$$\n",
    "\n",
    "Where:\n",
    "- **N** = Number of words in the sequence.\n",
    "- **P(x_i)** = Probability assigned by the model to the i-th word in the sequence.\n",
    "\n",
    "**Example**  \n",
    "Let’s consider the text: “I love to”  \n",
    "The model’s predictions for the next word are:\n",
    "- “eat” with a probability of 60% (0.60)\n",
    "- “sleep” with a probability of 30% (0.30)\n",
    "- “dance” with a probability of 10% (0.10)\n",
    "\n",
    "To calculate perplexity, we follow these steps:\n",
    "\n",
    "1. **Calculate the log probabilities**:\n",
    "   - For “eat”: \\( \\log_2(0.60) \\)\n",
    "   - For “sleep”: \\( \\log_2(0.30) \\)\n",
    "   - For “dance”: \\( \\log_2(0.10) \\)\n",
    "\n",
    "2. **Sum the log probabilities**:\n",
    "   $$ \\sum \\log_2 P(x_i) = \\log_2(0.60) + \\log_2(0.30) + \\log_2(0.10) $$\n",
    "\n",
    "3. **Average the log probabilities**:\n",
    "   $$ \\frac{1}{N} \\sum \\log_2 P(x_i) $$\n",
    "\n",
    "4. **Calculate the perplexity**:\n",
    "   $$ \\text{Perplexity} = 2^{\\left(-\\frac{1}{N} \\sum \\log_2 P(x_i)\\right)} $$\n",
    "\n",
    "Given the probabilities, the perplexity for this example is calculated to be **1.89**, which indicates a good prediction since lower perplexity values signify better model performance.\n",
    "\n",
    "\n",
    "\n",
    "**Use Cases**\n",
    "- Language model evaluation\n",
    "- Text prediction quality\n",
    "- Model comparison\n",
    "\n",
    "## 2. Semantic Similarity Metrics\n",
    "\n",
    "### Cosine Similarity\n",
    "\n",
    "**Definition**  \n",
    "Measures the cosine of the angle between two text vectors in a multi-dimensional space.\n",
    "\n",
    "**Formula**  \n",
    "$$\\text{Cosine Similarity} = \\frac{A \\cdot B}{||A|| \\, ||B||}$$\n",
    "\n",
    "Where:\n",
    "- **A \\cdot B** = Dot product of vectors\n",
    "- **||A||** = Length of vector A\n",
    "- **||B||** = Length of vector B\n",
    "\n",
    "**Example**  \n",
    "Text 1: \"I love dogs\"  \n",
    "Text 2: \"I like canines\"  \n",
    "Vector representation:\n",
    "- Text 1: [0.8, 0.6, 0.9]\n",
    "- Text 2: [0.7, 0.5, 0.8]  \n",
    "Cosine Similarity: 0.97 (very similar)\n",
    "\n",
    "**Use Cases**\n",
    "- Semantic search\n",
    "- Document similarity\n",
    "- Plagiarism detection\n",
    "\n",
    "### ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "\n",
    "**Definition**  \n",
    "ROUGE measures the quality of text summaries by comparing them with reference summaries.\n",
    "\n",
    "**Types**\n",
    "- **ROUGE-N**: N-gram overlap\n",
    "- **ROUGE-L**: Longest common subsequence\n",
    "- **ROUGE-W**: Weighted longest common subsequence\n",
    "\n",
    "**Formula (ROUGE-N)**  \n",
    "$$\\text{ROUGE-N} = \\frac{\\sum \\text{common n-grams}}{\\sum \\text{reference n-grams}}$$\n",
    "\n",
    "**Example**  \n",
    "Reference: \"The quick brown fox jumps\"  \n",
    "Summary: \"The brown fox jumps quickly\"  \n",
    "ROUGE-1: 0.75 (word overlap)  \n",
    "ROUGE-2: 0.50 (2-gram overlap)\n",
    "\n",
    "**Use Cases**\n",
    "- Text summarization\n",
    "- Content generation\n",
    "- Translation evaluation\n",
    "\n",
    "## 3. Language Quality Metrics\n",
    "\n",
    "### Grammar and Fluency Score\n",
    "\n",
    "**Definition**  \n",
    "Measures grammatical correctness and natural flow of text.\n",
    "\n",
    "**Components**\n",
    "\n",
    "**Grammar Score**  \n",
    "$$\\text{Grammar Score} = \\frac{\\text{Correct sentences}}{\\text{Total sentences}}$$\n",
    "\n",
    "**Fluency Score**  \n",
    "$$\\text{Fluency Score} = \\frac{\\text{Natural transitions}}{\\text{Total transitions}}$$\n",
    "\n",
    "**Example**  \n",
    "Text: \"I am going to store. The weather nice.\"  \n",
    "Grammar Score: 0.5 (1 correct / 2 sentences)  \n",
    "Fluency Score: 0.0 (unnatural transition)\n",
    "\n",
    "**Use Cases**\n",
    "- Content quality assessment\n",
    "- Educational applications\n",
    "- Professional writing tools\n",
    "\n",
    "## 4. Task-Specific Metrics\n",
    "\n",
    "### Question-Answering Metrics\n",
    "\n",
    "**Exact Match (EM)**  \n",
    "$$\\text{EM} = \\frac{\\text{Exact matches}}{\\text{Total questions}}$$\n",
    "\n",
    "**F1 Score**  \n",
    "$$\\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
    "\n",
    "Where:\n",
    "- **Precision** = Correct words / Predicted words\n",
    "- **Recall** = Correct words / Reference words\n",
    "\n",
    "**Example**  \n",
    "Question: \"What is the capital of France?\"  \n",
    "Correct: \"Paris\"  \n",
    "AI Answer: \"The capital is Paris\"  \n",
    "EM: 0 (not exact match)  \n",
    "F1: 0.67 (partial match)\n",
    "\n",
    "## 5. Human Evaluation Metrics\n",
    "\n",
    "### Expert Rating System\n",
    "\n",
    "**Components**\n",
    "- **Clarity** (1-5)\n",
    "- **Accuracy** (1-5)\n",
    "- **Usefulness** (1-5)\n",
    "\n",
    "**Formula**  \n",
    "$$\\text{Overall Score} = \\frac{\\text{Clarity} + \\text{Accuracy} + \\text{Usefulness}}{15} \\times 100$$\n",
    "\n",
    "**Example**  \n",
    "AI Text Evaluation:\n",
    "- Clarity: 4/5\n",
    "- Accuracy: 5/5\n",
    "- Usefulness: 4/5  \n",
    "Overall Score: 86.7%\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
